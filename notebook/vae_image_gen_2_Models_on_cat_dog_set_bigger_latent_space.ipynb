{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use to generate new data by manipulation\n",
    "\n",
    "## What is this?\n",
    "VAE stands for Variational Auto Encoder. Which means that is trained to reproduce or reconstruct from a latent space the input. Is an strategy to learn that latent space\n",
    "\n",
    "It has 2 important blocks the Encoder wich will be responsable for condensing the data into a low dimensional latent space or just a vector.\n",
    "And the Decoder by taking one point(how do you choose a point? By sampling) from the latent space is able to reconstruct a new image.\n",
    "\n",
    "## How do VAEs build a latent space?\n",
    "That is what difference a VAE from a AE that the V which means variational and allows the VAE to create continuous and structured latent spaces.\n",
    "\n",
    "## How do you sample the latent space?\n",
    "\n",
    "\n",
    "The latent space is structured, non sparse continuous and low dimentional where each direction encode a meanful axis of variation(V) of the data. And that means it can be manipulated with content vectors. Vectors that are isolated and represent a concept.\n",
    "Like the concept of smile another image representation can be added to the smile concept vector then passed to the decoder to create a new image with the person smiling.\n",
    "\n",
    "```There are concept vectors for any independent direction of the latent space```\n",
    "```deeplearning with bayesian inference```\n",
    "\n",
    "## Steps\n",
    "- Build the encoder\n",
    "    - The output will be 2 vectors mean and variance\n",
    "- Build the sampler using a random small vector along with the 2 vectors the encoder will give us\n",
    "    - normal dist value = mean * exp(std) * epsilon / epsilon is a random small vector from the latent space\n",
    "- Build the loss\n",
    "    - kubell-divergence\n",
    "    - reconstruction loss mean already coded in keras\n",
    "- Build the decoder\n",
    "    - the decoder will take the sampled input and reconstruct it to a valid image!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "\n",
    "The encoder will transform the image into 2 parameters vectors that will be used to form a normal distribution, mean_vector and standard_deviation_vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy dataset Nmist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = IMAGE_WIDTH = 64\n",
    "DATASET = \"monet\"\n",
    "# DATASET = \"celebs\"\n",
    "# DATASET = \"cat_vs_dogs\"\n",
    "COLOR_MODE = \"rgb\"\n",
    "\n",
    "if DATASET == \"cat_vs_dogs\":\n",
    "  whole_dataset = tf.keras.utils.image_dataset_from_directory(\"./PetImages/\",\n",
    "                                                              label_mode=None,\n",
    "                                                                image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                                                                  batch_size=256,\n",
    "                                                                  shuffle=True,\n",
    "                                                                  color_mode=COLOR_MODE,\n",
    "                                                                  smart_resize=True)\n",
    "\n",
    "if DATASET == \"celebs\":\n",
    "  whole_dataset = tf.keras.utils.image_dataset_from_directory(\"./Celeb_images/\",\n",
    "                                                              label_mode=None,\n",
    "                                                                image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                                                                  batch_size=256,\n",
    "                                                                  shuffle=True,\n",
    "                                                                  color_mode=COLOR_MODE,\n",
    "                                                                  smart_resize=True)\n",
    "  \n",
    "if DATASET == \"monet\":\n",
    "  whole_dataset = tf.keras.utils.image_dataset_from_directory(\"./art_images/monet_jpg/\",\n",
    "                                                              label_mode=None,\n",
    "                                                                image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "                                                                  batch_size=256,\n",
    "                                                                  shuffle=True,\n",
    "                                                                  color_mode=COLOR_MODE,\n",
    "                                                                  smart_resize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for batch in whole_dataset:\n",
    "    for image in batch:\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dataset = whole_dataset.map(lambda x: x/255., num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIMS = 2\n",
    "IMAGE_CHANNELS = 3 if COLOR_MODE == \"rgb\" else 1\n",
    "PROY_DIM = 128 #16\n",
    "\n",
    "# the input is the output of the sample(encoder(image))\n",
    "# CHANNELS_ENCODER_OUTPUT = 256\n",
    "CONVS_NUMBER_ENCODER = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to use a fix size.. This should not be a limitation in the future.\n",
    "# If I create a class I can surpass that limitation\n",
    "X_input_encoder = tf.keras.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n",
    "filters_decoder = 16\n",
    "\n",
    "# add more convs\n",
    "#Last conv\n",
    "X = tf.keras.layers.Conv2D(filters=filters_decoder, kernel_size=3, activation=\"relu\", strides=2, padding=\"same\")(X_input_encoder)\n",
    "\n",
    "for channel_index in range(CONVS_NUMBER_ENCODER-1):\n",
    "    filters_decoder *= 2\n",
    "    X = tf.keras.layers.Conv2D(filters=filters_decoder, kernel_size=3, activation=\"relu\", strides=2, padding=\"same\")(X)\n",
    "    CHANNELS_ENCODER_OUTPUT = filters_decoder\n",
    "    #Adding more convs diminish kl divergence at first then increases it??\n",
    "\n",
    "X = tf.keras.layers.Flatten()(X)\n",
    "\n",
    "# Try a proyection model sequence latter\n",
    "X = tf.keras.layers.Dense(units=PROY_DIM, activation=\"relu\")(X)\n",
    "# X = tf.keras.Sequential([tf.keras.layers.Dense(units=PROY_DIM, activation=\"relu\"), tf.keras.layers.Dense(units=PROY_DIM)])(X)\n",
    "\n",
    "z_mean = tf.keras.layers.Dense(units=LATENT_DIMS, name=\"z_mean\")(X)\n",
    "z_log_var = tf.keras.layers.Dense(units=LATENT_DIMS, name=\"z_log_var\")(X)\n",
    "\n",
    "encoder = tf.keras.Model(inputs=X_input_encoder, outputs=[z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampler!\n",
    "\n",
    "We are sampling from a normal distribution\n",
    "\n",
    "normal = mu + exp(sigma) * epsilon\n",
    "\n",
    "epsilon is a random number from a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(tf.keras.layers.Layer):\n",
    "    def call(self, z_mean, z_sigma):\n",
    "        # get the batch size\n",
    "        batch_size = tf.shape(z_mean)[0]\n",
    "        latent_dim = tf.shape(z_mean)[1]\n",
    "\n",
    "        #Epsilon should be the same size as our vectors\n",
    "        # here we are in the training and everything gets processed in batch\n",
    "        epsilon = tf.random.normal(shape=(batch_size, latent_dim))\n",
    "\n",
    "        #This returns a sample point from the distribution we are trying to find.\n",
    "        # A normal distribution\n",
    "        # Why over 2? is that 2 the N elements?\n",
    "        return z_mean + tf.math.exp(z_sigma/2) * epsilon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image is 256 but we have 2 transpose layers the image will get upscaled 2 times and we only applied 1 layer of convs with strides 2 \n",
    "# At this point the image is 128 but if we are going to upscale 2 times for the image to be again 256 it should be 64\n",
    "CONVS_NUMBER = 3\n",
    "IMAGE_HEIGHT_DECODE = IMAGE_WIDTH_DECODE = math.ceil(IMAGE_HEIGHT/(2**CONVS_NUMBER))\n",
    "\n",
    "X_input_decoder = tf.keras.Input(shape=(LATENT_DIMS,))\n",
    "\n",
    "X = tf.keras.layers.Dense(units=IMAGE_HEIGHT_DECODE * IMAGE_WIDTH_DECODE * CHANNELS_ENCODER_OUTPUT)(X_input_decoder)\n",
    "X = tf.keras.layers.Reshape((IMAGE_HEIGHT_DECODE, IMAGE_WIDTH_DECODE, CHANNELS_ENCODER_OUTPUT))(X)\n",
    "\n",
    "filters = CHANNELS_ENCODER_OUTPUT\n",
    "for channel_index in range(CONVS_NUMBER):\n",
    "    X = tf.keras.layers.Conv2DTranspose(filters, 3, activation=\"relu\", strides=2, padding=\"same\")(X)\n",
    "    filters = math.ceil(filters/2)\n",
    "\n",
    "X_decoder_output = tf.keras.layers.Conv2D(IMAGE_CHANNELS, 3, activation=\"sigmoid\", padding=\"same\")(X)\n",
    "\n",
    "#X_input_decoder latent input\n",
    "decoder = tf.keras.Model(X_input_decoder, X_decoder_output, name=\"decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = tf.constant(np.random.normal(size=(8, 5, 4, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum(sample, axis=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(tf.reduce_sum(sample, axis=(1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, sampler, **kwars):\n",
    "        \"\"\" \n",
    "         It has 3 main blocks \n",
    "            - encoder\n",
    "            - decoder\n",
    "            - sampler\n",
    "        \"\"\"\n",
    "        super().__init__(**kwars)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.sampler = sampler\n",
    "\n",
    "        # sum all losses\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_flat_loss\")\n",
    "\n",
    "        # how close the reconstructed sample by the decoder is to the original source\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "\n",
    "        # divergence from the distribution created to model the latent space and the real one which is a normal distribution\n",
    "        # From where the epsilon point comes from?\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_sigma = self.encoder(inputs)\n",
    "        return self.decoder(self.sampler(z_mean, z_sigma))\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.total_loss_tracker,\n",
    "                self.reconstruction_loss_tracker,\n",
    "                self.kl_loss_tracker]\n",
    "    \n",
    "    def train_step(self, batch_data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_sigma = self.encoder(batch_data)\n",
    "            sampled_point_z = self.sampler(z_mean, z_sigma)\n",
    "            reconstructed_data = self.decoder(sampled_point_z)\n",
    "\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                # The image needs to be normalized between 0 and 1 for this to make sense\n",
    "                # binary difference between input data and reconstructed\n",
    "                tf.reduce_sum(tf.keras.losses.binary_crossentropy(batch_data, reconstructed_data),\n",
    "                              # sum by sample, reduce the whole row to 1 value\n",
    "                              axis=(1, 2))\n",
    "            )\n",
    "\n",
    "            kl_loss = -0.5 * (1 + z_sigma - tf.math.square(z_mean) - tf.math.exp(z_sigma))\n",
    "\n",
    "            # this adds all 2 losses into one\n",
    "            # how well the input was reconstructed and the distribution difference to create the latent space\n",
    "            total_loss = reconstruction_loss + tf.reduce_mean(kl_loss)\n",
    "\n",
    "        # Standard way to propagate the error signal?\n",
    "        gradients = tape.gradient(total_loss, self.trainable_weights)\n",
    "        # gradients = tape.gradient(total_loss, [z_mean, z_sigma])\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
    "        # self.optimizer.apply_gradients(zip(gradients, [z_mean, z_sigma]))\n",
    "\n",
    "        # add and average the loss so far up to this batch\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return {\n",
    "            \"total_loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder=encoder, decoder=decoder, sampler=Sampler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.compile(optimizer=tf.keras.optimizers.legacy.Adam(), run_eagerly=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: add a callback to preview the output of the generator vs the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEMonitor(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=LATENT_DIMS):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "  \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(\n",
    "            shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.decoder(random_latent_vectors)\n",
    "        generated_images *= 255 \n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = tf.keras.utils.array_to_img(generated_images[i])\n",
    "            img.save(f\"./celeb_vae_gen/generated_img_{epoch:03d}_{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.fit(whole_dataset, epochs=30, callbacks=[VAEMonitor(num_img=10, latent_dim=LATENT_DIMS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(whole_dataset))[0]\n",
    "sample = np.expand_dims(sample, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed = vae.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed[0].shape\n",
    "reconstructed = reconstructed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed *= 128\n",
    "reconstructed += 64\n",
    "reconstructed = np.clip(0, 255, reconstructed)\n",
    "reconstructed = reconstructed.astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, var = encoder(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Sampler()(mu, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((decoder(tf.constant([[.2, -0.2]], dtype=\"float32\"))[0].numpy()*255).astype(\"uint8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((decoder(z)[0].numpy()*255).astype(\"uint8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = encoder(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
