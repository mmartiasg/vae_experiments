{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use to generate new data by manipulation\n",
    "\n",
    "## What is this?\n",
    "VAE stands for Variational Auto Encoder. Which means that is trained to reproduce or reconstruct from a latent space the input. Is an strategy to learn that latent space\n",
    "\n",
    "It has 2 important blocks the Encoder wich will be responsable for condensing the data into a low dimensional latent space or just a vector.\n",
    "And the Decoder by taking one point(how do you choose a point? By sampling) from the latent space is able to reconstruct a new image.\n",
    "\n",
    "## How do VAEs build a latent space?\n",
    "That is what difference a VAE from a AE that the V which means variational and allows the VAE to create continuous and structured latent spaces.\n",
    "\n",
    "## How do you sample the latent space?\n",
    "\n",
    "\n",
    "The latent space is structured, non sparse continuous and low dimentional where each direction encode a meanful axis of variation(V) of the data. And that means it can be manipulated with content vectors. Vectors that are isolated and represent a concept.\n",
    "Like the concept of smile another image representation can be added to the smile concept vector then passed to the decoder to create a new image with the person smiling.\n",
    "\n",
    "```There are concept vectors for any independent direction of the latent space```\n",
    "```deeplearning with bayesian inference```\n",
    "\n",
    "## Steps\n",
    "- Build the encoder\n",
    "    - The output will be 2 vectors mean and variance\n",
    "- Build the sampler using a random small vector along with the 2 vectors the encoder will give us\n",
    "    - normal dist value = mean * exp(std) * epsilon / epsilon is a random small vector from the latent space\n",
    "- Build the loss\n",
    "    - kubell-divergence\n",
    "    - reconstruction loss mean already coded in keras\n",
    "- Build the decoder\n",
    "    - the decoder will take the sampled input and reconstruct it to a valid image!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "\n",
    "The encoder will transform the image into 2 parameters vectors that will be used to form a normal distribution, mean_vector and standard_deviation_vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy dataset Nmist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, _), (X_test, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "mnist_digits = np.concatenate([X_train, X_test])\n",
    "mnist_digits = mnist_digits[:, :, :, np.newaxis]/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 28, 28, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_digits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEncoder(tf.keras.layers.Layer):    \n",
    "    def __init__(self, latent_dims=2, **kwars):\n",
    "        super().__init__(**kwars)\n",
    "        self.latent_dims = latent_dims\n",
    "        \n",
    "        self.z_mean = tf.keras.layers.Dense(units=latent_dims, activation=\"selu\")\n",
    "        self.z_sigma = tf.keras.layers.Dense(units=latent_dims, activation=\"selu\")\n",
    "\n",
    "        # with padding same the activation map will have the same size\n",
    "        self.conv_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\")\n",
    "\n",
    "        # with padding same the activation map will have the same size\n",
    "        self.conv_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\")\n",
    "\n",
    "        # Proyection\n",
    "        self.proy = tf.keras.Sequential([tf.keras.layers.Dense(units=16, activation=\"relu\"), tf.keras.layers.Dense(units=16)])\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        x = self.conv_1(inputs)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.proy(x)\n",
    "        return self.z_mean(x), self.z_sigma(x)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\"latent_dims\":self.latent_dims}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = VAEncoder(latent_dims=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.expand_dims(mnist_digits[0], axis=0)\n",
    "z_mean, z_sigma = encoder(sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampler!\n",
    "\n",
    "We are sampling from a normal distribution\n",
    "\n",
    "normal = mu + exp(sigma) * epsilon\n",
    "\n",
    "epsilon is a random number from a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(tf.keras.layers.Layer):\n",
    "    def call(self, z_mean, z_sigma):\n",
    "        # get the batch size\n",
    "        batch_size = tf.shape(z_mean.shape)[0]\n",
    "        latent_dim = tf.shape(z_mean)[1]\n",
    "\n",
    "        #Epsilon should be the same size as our vectors\n",
    "        # here we are in the training and everything gets processed in batch\n",
    "        epsilon = tf.random.normal(shape=(batch_size, latent_dim))\n",
    "\n",
    "        #This returns a sample point from the distribution we are trying to find.\n",
    "        # A normal distribution\n",
    "        # Why over 2?\n",
    "        return z_mean + tf.math.exp(z_sigma/2) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-1.3263273 ,  0.74788636],\n",
       "       [ 0.11071271,  0.9165265 ]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sampler()(z_mean, z_sigma)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_mean = tf.expand_dims(z_mean, axis=0)\n",
    "# z_sigma = tf.expand_dims(z_sigma, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, latent_dims=2, image_original_widht=7, image_original_height=7, image_original_channels=1, last_conv_channels=64, trainable=True, name=None, dtype=None, dynamic=False, **kwargs):\n",
    "        super().__init__(trainable, name, dtype, dynamic, **kwargs)\n",
    "        self.latent_dims = latent_dims\n",
    "        self.image_original_widht = image_original_widht\n",
    "        self.image_original_height = image_original_height\n",
    "        self.image_original_channels = image_original_channels\n",
    "        self.last_conv_channels = last_conv_channels\n",
    "\n",
    "        # image flatten size\n",
    "        # 7 * 7 * 1= 49\n",
    "        self.latent_space = tf.keras.layers.Dense(units=self.image_original_widht*self.image_original_height*self.last_conv_channels, activation=\"relu\")\n",
    "        self.reshape = tf.keras.layers.Reshape((self.image_original_height, self.image_original_widht, self.last_conv_channels))\n",
    "        self.conv_traspose_1 = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\")\n",
    "        self.conv_traspose_2 = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\")\n",
    "        self.conv_image_gen = tf.keras.layers.Conv2D(filters=self.image_original_channels, kernel_size=3, padding=\"same\", activation=\"sigmoid\" if self.image_original_channels==1 else \"softmax\")\n",
    "\n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        x = self.latent_space(inputs)\n",
    "        x = self.reshape(x)\n",
    "        x = self.conv_traspose_1(x)\n",
    "        x = self.conv_traspose_2(x)\n",
    "        x = self.conv_image_gen(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "sampled_point = Sampler()(z_mean, z_sigma)\n",
    "print(sampled_point.shape)\n",
    "# Batch 1 sample\n",
    "sampled_point = tf.expand_dims(sampled_point, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'reshape_4' (type Reshape).\n\n{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 6272 values, but the requested shape has 3136 [Op:Reshape]\n\nCall arguments received by layer 'reshape_4' (type Reshape):\n  • inputs=tf.Tensor(shape=(1, 2, 3136), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m VAEDecoder()(sampled_point)\n",
      "File \u001b[0;32m~/miniforge3/envs/tfm1/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[26], line 20\u001b[0m, in \u001b[0;36mVAEDecoder.call\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     19\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatent_space(inputs)\n\u001b[0;32m---> 20\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreshape(x)\n\u001b[1;32m     21\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_traspose_1(x)\n\u001b[1;32m     22\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_traspose_2(x)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'reshape_4' (type Reshape).\n\n{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 6272 values, but the requested shape has 3136 [Op:Reshape]\n\nCall arguments received by layer 'reshape_4' (type Reshape):\n  • inputs=tf.Tensor(shape=(1, 2, 3136), dtype=float32)"
     ]
    }
   ],
   "source": [
    "VAEDecoder()(sampled_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
